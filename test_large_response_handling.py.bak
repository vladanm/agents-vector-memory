#!/usr/bin/env python3
"""
Comprehensive Test Suite for MCP Large Response Handling
========================================================

This test suite validates the complete implementation of large document handling
including the write_document_to_file tool and size warning features.

Test Coverage:
1. Small Document (Direct Response)
2. Large Document (File Write)
3. Auto-Generated Path
4. Custom Path
5. Invalid Memory ID
6. Chunked Document Reconstruction
7. Metadata Frontmatter
8. Token Estimation Accuracy
9. Error Code Coverage
10. Integration with Search Tools
"""

import sys
from pathlib import Path
import tempfile
import shutil
import os
import json

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from session_memory_store import SessionMemoryStore


class TestLargeResponseHandling:
    """Comprehensive test suite for large response handling."""

    def __init__(self):
        """Initialize test suite with database connection."""
        # Use actual test database
        self.db_path = Path("/Users/vladanm/projects/subagents/simple-agents/memory/memory/agent_session_memory.db")

        if not self.db_path.exists():
            raise FileNotFoundError(f"Test database not found at {self.db_path}")

        self.store = SessionMemoryStore(db_path=self.db_path)

        # Test data storage
        self.test_memory_ids = []
        self.test_session_id = "test-large-response-handling"
        self.test_agent_id = "test-agent"

        # Create temp directory for test outputs
        self.temp_dir = Path(tempfile.gettempdir()) / "test_large_response_handling"
        self.temp_dir.mkdir(parents=True, exist_ok=True)

    def cleanup(self):
        """Clean up test files and data."""
        # Clean up temp directory
        if self.temp_dir.exists():
            shutil.rmtree(self.temp_dir, ignore_errors=True)

        print(f"\n🧹 Cleaned up test directory: {self.temp_dir}")

    # ===================================================================
    # Helper Methods
    # ===================================================================

    def create_small_document(self) -> str:
        """Create a document <20k tokens (~5000 tokens)."""
        return "This is a small test document with various content. " * 300

    def create_large_document(self) -> str:
        """Create a document >20k tokens (~45000 tokens)."""
        # Create realistic large document content
        sections = []
        for i in range(50):
            sections.append(f"""
## Section {i + 1}

This is section {i + 1} of a large technical document. It contains detailed
information about various implementation aspects, including architecture
decisions, code patterns, performance considerations, and best practices.

### Subsection {i + 1}.1

Here we discuss the first major topic in this section, covering implementation
details, edge cases, error handling, and integration patterns.

### Subsection {i + 1}.2

The second subsection covers additional considerations, including testing
strategies, deployment procedures, monitoring approaches, and maintenance tasks.

### Subsection {i + 1}.3

Finally, we examine advanced topics such as optimization techniques,
scalability patterns, security considerations, and future enhancements.
""")

        return "\n\n".join(sections)

    def create_medium_document(self) -> str:
        """Create a document ~15k tokens."""
        return "This is a medium-sized test document with substantial content. " * 1500

    def store_test_memory(self, content: str, title: str, memory_type: str = "knowledge_base",
                         auto_chunk: bool = False) -> dict:
        """Store a test memory and track its ID."""
        result = self.store.store_memory(
            memory_type=memory_type,
            agent_id=self.test_agent_id,
            session_id=self.test_session_id,
            content=content,
            title=title,
            session_iter=1,
            auto_chunk=auto_chunk,
            tags=["test", "large-response-handling"]
        )

        if result.get("success") and result.get("memory_id"):
            self.test_memory_ids.append(result["memory_id"])

        return result

    # ===================================================================
    # Test Case 1: Small Document (Direct Response)
    # ===================================================================

    def test_small_document_full_content(self) -> bool:
        """
        Test Case 1: Small Document (Direct Response)

        - Search returns document <20k tokens
        - Full content included in response
        - exceeds_response_limit: false
        - size_warning: null
        - Agent can read content directly
        """
        print("\n" + "="*70)
        print("TEST CASE 1: Small Document (Direct Response)")
        print("="*70)

        try:
            # Store small document
            small_content = self.create_small_document()
            store_result = self.store_test_memory(
                content=small_content,
                title="Small Test Document",
                memory_type="knowledge_base",
                auto_chunk=False
            )

            memory_id = store_result["memory_id"]
            print(f"✓ Stored small document (memory_id: {memory_id})")
            print(f"  Content length: {len(small_content)} characters")

            # Search for document
            search_result = self.store.search_with_granularity(
                query="small test document",
                memory_type="knowledge_base",
                granularity="coarse",
                agent_id=self.test_agent_id,
                session_id=self.test_session_id,
                limit=5
            )

            assert search_result["success"], "Search should succeed"
            assert len(search_result["results"]) > 0, "Should return results"

            # Find our document in results
            doc = None
            for result in search_result["results"]:
                if result["memory_id"] == memory_id:
                    doc = result
                    break

            assert doc is not None, f"Should find document with memory_id {memory_id}"

            # Verify all fields
            print(f"\n  Response fields:")
            print(f"    - estimated_tokens: {doc.get('estimated_tokens')}")
            print(f"    - exceeds_response_limit: {doc.get('exceeds_response_limit')}")
            print(f"    - size_warning: {doc.get('size_warning')}")
            print(f"    - content_length: {len(doc.get('content', ''))}")

            # Assertions
            assert doc.get("estimated_tokens") is not None, "Should have estimated_tokens"
            assert doc.get("estimated_tokens") < 20000, "Should be < 20k tokens"
            assert doc.get("exceeds_response_limit") == False, "Should not exceed limit"
            assert doc.get("size_warning") is None, "Should not have size warning"
            assert len(doc.get("content", "")) > 1000, "Should return full content"
            assert doc.get("content") == small_content, "Content should match original"

            print(f"\n✅ TEST CASE 1 PASSED: Small document returns full content")
            return True

        except Exception as e:
            print(f"\n✗ TEST CASE 1 FAILED: {str(e)}")
            import traceback
            traceback.print_exc()
            return False

    # ===================================================================
    # Test Case 2: Large Document (File Write)
    # ===================================================================

    def test_large_document_with_warning(self) -> bool:
        """
        Test Case 2: Large Document (File Write)

        - Search returns document >20k tokens
        - Preview content (500 chars) returned
        - exceeds_response_limit: true
        - size_warning object with recommendation
        - Agent uses write_document_to_file
        - Agent reads file successfully
        """
        print("\n" + "="*70)
        print("TEST CASE 2: Large Document (File Write)")
        print("="*70)

        try:
            # Store large document
            large_content = self.create_large_document()
            store_result = self.store_test_memory(
                content=large_content,
                title="Large Test Document",
                memory_type="reports",
                auto_chunk=False
            )

            memory_id = store_result["memory_id"]
            print(f"✓ Stored large document (memory_id: {memory_id})")
            print(f"  Content length: {len(large_content)} characters")

            # Search for document
            search_result = self.store.search_with_granularity(
                query="large test document",
                memory_type="reports",
                granularity="coarse",
                agent_id=self.test_agent_id,
                session_id=self.test_session_id,
                limit=5
            )

            assert search_result["success"], "Search should succeed"

            # Find our document
            doc = None
            for result in search_result["results"]:
                if result["memory_id"] == memory_id:
                    doc = result
                    break

            assert doc is not None, f"Should find document with memory_id {memory_id}"

            # Verify size warning fields
            print(f"\n  Response fields:")
            print(f"    - estimated_tokens: {doc.get('estimated_tokens')}")
            print(f"    - exceeds_response_limit: {doc.get('exceeds_response_limit')}")
            print(f"    - size_warning: {json.dumps(doc.get('size_warning'), indent=6)}")
            print(f"    - content_preview_length: {len(doc.get('content', ''))}")

            # Assertions for search response
            assert doc.get("estimated_tokens") is not None, "Should have estimated_tokens"
            assert doc.get("estimated_tokens") > 20000, f"Should be > 20k tokens, got {doc.get('estimated_tokens')}"
            assert doc.get("exceeds_response_limit") == True, "Should exceed limit"
            assert doc.get("size_warning") is not None, "Should have size warning"

            size_warning = doc["size_warning"]
            assert size_warning.get("is_too_large") == True, "Warning should indicate too large"
            assert size_warning.get("document_tokens") > 20000, "Should show actual token count"
            assert "write_document_to_file" in size_warning.get("recommended_action", ""), \
                "Should recommend write_document_to_file"

            # Content should be preview, not full
            assert len(doc.get("content", "")) < len(large_content), "Should return preview, not full content"
            assert "[Content truncated" in doc.get("content", ""), "Preview should indicate truncation"

            print(f"\n✓ Search response validated - size warning present")

            # Now test write_document_to_file
            print(f"\n  Testing write_document_to_file...")

            write_result = self.store.write_document_to_file(
                memory_id=memory_id,
                output_path=None,  # Auto-generate path
                include_metadata=True
            )

            assert write_result["success"], "Write should succeed"
            assert "file_path" in write_result, "Should return file_path"
            assert write_result.get("estimated_tokens") > 20000, "Should show token count"

            file_path = write_result["file_path"]
            print(f"    - file_path: {file_path}")
            print(f"    - file_size: {write_result.get('file_size_human')}")
            print(f"    - estimated_tokens: {write_result.get('estimated_tokens')}")

            # Verify file exists and is readable
            assert Path(file_path).exists(), "File should exist"

            with open(file_path, 'r', encoding='utf-8') as f:
                file_content = f.read()

            assert len(file_content) > len(large_content), "File should include metadata"
            assert large_content in file_content, "File should contain full content"

            print(f"\n✓ File written and verified")
            print(f"✅ TEST CASE 2 PASSED: Large document warning + file write successful")
            return True

        except Exception as e:
            print(f"\n✗ TEST CASE 2 FAILED: {str(e)}")
            import traceback
            traceback.print_exc()
            return False

    # ===================================================================
    # Test Case 3: Auto-Generated Path
    # ===================================================================

    def test_auto_generated_path(self) -> bool:
        """
        Test Case 3: Auto-Generated Path

        - Call write_document_to_file without output_path
        - Temp path generated automatically
        - Path uses tempfile.gettempdir() + /vector_memory/
        - File created successfully
        - File is readable
        """
        print("\n" + "="*70)
        print("TEST CASE 3: Auto-Generated Path")
        print("="*70)

        try:
            # Create and store document
            content = self.create_medium_document()
            store_result = self.store_test_memory(
                content=content,
                title="Auto Path Test Document",
                memory_type="knowledge_base"
            )

            memory_id = store_result["memory_id"]
            print(f"✓ Stored document (memory_id: {memory_id})")

            # Write with auto-generated path (output_path=None)
            write_result = self.store.write_document_to_file(
                memory_id=memory_id,
                output_path=None,
                include_metadata=False
            )

            assert write_result["success"], "Write should succeed"

            file_path = write_result["file_path"]
            print(f"\n  Auto-generated path: {file_path}")

            # Verify path structure
            file_path_obj = Path(file_path)
            assert file_path_obj.is_absolute(), "Path should be absolute"
            assert "vector_memory" in str(file_path), "Path should contain 'vector_memory'"
            assert file_path_obj.exists(), "File should exist"

            # Verify in temp directory
            temp_base = Path(tempfile.gettempdir())
            assert str(file_path_obj).startswith(str(temp_base)), \
                f"Path should be in temp directory ({temp_base})"

            # Verify file is readable
            with open(file_path, 'r', encoding='utf-8') as f:
                file_content = f.read()

            assert len(file_content) > 0, "File should have content"
            assert content in file_content, "File should contain document content"

            print(f"  ✓ Path is absolute: {file_path_obj.is_absolute()}")
            print(f"  ✓ In temp directory: {str(file_path_obj).startswith(str(temp_base))}")
            print(f"  ✓ File exists: {file_path_obj.exists()}")
            print(f"  ✓ File readable: True")

            print(f"\n✅ TEST CASE 3 PASSED: Auto-generated path works correctly")
            return True

        except Exception as e:
            print(f"\n✗ TEST CASE 3 FAILED: {str(e)}")
            import traceback
            traceback.print_exc()
            return False

    # ===================================================================
    # Test Case 4: Custom Path
    # ===================================================================

    def test_custom_path(self) -> bool:
        """
        Test Case 4: Custom Path

        - Call write_document_to_file with custom output_path
        - Directory created if needed
        - File written to specified location
        - Path validation works
        """
        print("\n" + "="*70)
        print("TEST CASE 4: Custom Path")
        print("="*70)

        try:
            # Create and store document
            content = self.create_small_document()
            store_result = self.store_test_memory(
                content=content,
                title="Custom Path Test Document",
                memory_type="reports"
            )

            memory_id = store_result["memory_id"]
            print(f"✓ Stored document (memory_id: {memory_id})")

            # Create custom path in our test directory
            custom_dir = self.temp_dir / "custom_output"
            custom_path = custom_dir / "test_custom_document.md"

            print(f"\n  Custom path: {custom_path}")
            print(f"  Directory exists before: {custom_dir.exists()}")

            # Write with custom path
            write_result = self.store.write_document_to_file(
                memory_id=memory_id,
                output_path=str(custom_path),
                include_metadata=True
            )

            assert write_result["success"], "Write should succeed"
            assert write_result["file_path"] == str(custom_path), \
                f"Returned path should match requested path"

            print(f"  Directory exists after: {custom_dir.exists()}")
            print(f"  File exists: {custom_path.exists()}")

            # Verify directory was created
            assert custom_dir.exists(), "Directory should be created"

            # Verify file exists at custom location
            assert custom_path.exists(), "File should exist at custom path"

            # Verify file content
            with open(custom_path, 'r', encoding='utf-8') as f:
                file_content = f.read()

            assert len(file_content) > 0, "File should have content"
            assert content in file_content, "File should contain document content"
            assert "memory_id:" in file_content, "File should include metadata (YAML frontmatter)"

            print(f"\n  ✓ Directory created: {custom_dir.exists()}")
            print(f"  ✓ File at correct location: {custom_path.exists()}")
            print(f"  ✓ Content verified: True")
            print(f"  ✓ Metadata included: {'memory_id:' in file_content}")

            print(f"\n✅ TEST CASE 4 PASSED: Custom path works correctly")
            return True

        except Exception as e:
            print(f"\n✗ TEST CASE 4 FAILED: {str(e)}")
            import traceback
            traceback.print_exc()
            return False

    # ===================================================================
    # Test Case 5: Invalid Memory ID
    # ===================================================================

    def test_invalid_memory_id(self) -> bool:
        """
        Test Case 5: Invalid Memory ID

        - Call write_document_to_file with non-existent memory_id
        - Returns error with code MEMORY_NOT_FOUND
        - Error message is clear
        - No file created
        """
        print("\n" + "="*70)
        print("TEST CASE 5: Invalid Memory ID")
        print("="*70)

        try:
            # Use a memory_id that definitely doesn't exist
            invalid_memory_id = 999999999

            print(f"  Testing with invalid memory_id: {invalid_memory_id}")

            # Try to write document with invalid ID
            write_result = self.store.write_document_to_file(
                memory_id=invalid_memory_id,
                output_path=None,
                include_metadata=True
            )

            print(f"\n  Response:")
            print(f"    - success: {write_result.get('success')}")
            print(f"    - error: {write_result.get('error')}")
            print(f"    - message: {write_result.get('message')}")

            # Verify error response
            assert write_result["success"] == False, "Should return failure"
            assert write_result.get("error") == "MEMORY_NOT_FOUND", \
                f"Error code should be MEMORY_NOT_FOUND, got {write_result.get('error')}"
            assert "message" in write_result, "Should include error message"
            assert str(invalid_memory_id) in write_result.get("message", ""), \
                "Error message should mention the memory_id"

            # Verify no file was created
            assert "file_path" not in write_result, "Should not return file_path on error"

            print(f"\n  ✓ Error code correct: MEMORY_NOT_FOUND")
            print(f"  ✓ Error message clear: Yes")
            print(f"  ✓ No file created: Yes")

            print(f"\n✅ TEST CASE 5 PASSED: Invalid memory ID handled correctly")
            return True

        except Exception as e:
            print(f"\n✗ TEST CASE 5 FAILED: {str(e)}")
            import traceback
            traceback.print_exc()
            return False

    # ===================================================================
    # Test Case 6: Chunked Document Reconstruction
    # ===================================================================

    def test_chunked_document_reconstruction(self) -> bool:
        """
        Test Case 6: Chunked Document Reconstruction

        - Document stored with auto_chunked=true
        - write_document_to_file reconstructs correctly
        - All chunks included
        - No data loss
        - Proper spacing between chunks
        """
        print("\n" + "="*70)
        print("TEST CASE 6: Chunked Document Reconstruction")
        print("="*70)

        try:
            # Create large document that will be chunked
            chunked_content = self.create_large_document()

            print(f"  Original content length: {len(chunked_content)} characters")

            # Store with auto_chunk=True
            store_result = self.store_test_memory(
                content=chunked_content,
                title="Chunked Test Document",
                memory_type="knowledge_base",
                auto_chunk=True  # Enable chunking
            )

            memory_id = store_result["memory_id"]
            print(f"✓ Stored chunked document (memory_id: {memory_id})")

            # Check if chunks were created
            conn = self.store._get_connection()
            chunks = conn.execute("""
                SELECT chunk_index, LENGTH(content) as chunk_length
                FROM memory_chunks
                WHERE parent_id = ?
                ORDER BY chunk_index ASC
            """, (memory_id,)).fetchall()
            conn.close()

            chunk_count = len(chunks)
            print(f"  Chunks created: {chunk_count}")

            if chunk_count > 0:
                for idx, length in chunks:
                    print(f"    - Chunk {idx}: {length} characters")
            else:
                print("  ⚠ Warning: No chunks created (document may be too small)")

            # Reconstruct document using write_document_to_file
            write_result = self.store.write_document_to_file(
                memory_id=memory_id,
                output_path=None,
                include_metadata=False
            )

            assert write_result["success"], "Write should succeed"

            file_path = write_result["file_path"]
            print(f"\n✓ Document reconstructed to: {file_path}")

            # Read reconstructed content
            with open(file_path, 'r', encoding='utf-8') as f:
                reconstructed_content = f.read()

            reconstructed_length = len(reconstructed_content)
            original_length = len(chunked_content)

            print(f"\n  Content comparison:")
            print(f"    - Original length: {original_length} characters")
            print(f"    - Reconstructed length: {reconstructed_length} characters")
            print(f"    - Length difference: {abs(reconstructed_length - original_length)} characters")

            # Verify reconstruction quality
            # Allow for some spacing differences (chunk joins may add newlines)
            length_difference = abs(reconstructed_length - original_length)
            max_allowed_difference = chunk_count * 4 if chunk_count > 0 else 10

            assert length_difference <= max_allowed_difference, \
                f"Length difference too large: {length_difference} > {max_allowed_difference}"

            # Check that major content sections are present
            # Extract first 500 chars from original
            original_start = chunked_content[:500].strip()
            assert original_start in reconstructed_content, \
                "Beginning of original content should be in reconstruction"

            # Extract last 500 chars from original
            original_end = chunked_content[-500:].strip()
            assert original_end in reconstructed_content, \
                "End of original content should be in reconstruction"

            print(f"\n  ✓ Length difference acceptable: {length_difference} <= {max_allowed_difference}")
            print(f"  ✓ Beginning preserved: Yes")
            print(f"  ✓ End preserved: Yes")
            print(f"  ✓ No data loss detected")

            print(f"\n✅ TEST CASE 6 PASSED: Chunked document reconstructed correctly")
            return True

        except Exception as e:
            print(f"\n✗ TEST CASE 6 FAILED: {str(e)}")
            import traceback
            traceback.print_exc()
            return False

    # ===================================================================
    # Test Case 7: Metadata Frontmatter
    # ===================================================================

    def test_metadata_frontmatter(self) -> bool:
        """
        Test Case 7: Metadata Frontmatter

        - write_document_to_file with include_metadata=true
        - YAML frontmatter generated
        - Contains memory_id, title, tags, etc.
        - Valid YAML format
        - Content follows frontmatter
        """
        print("\n" + "="*70)
        print("TEST CASE 7: Metadata Frontmatter")
        print("="*70)

        try:
            # Create document with rich metadata
            content = self.create_small_document()

            store_result = self.store_test_memory(
                content=content,
                title="Metadata Test Document",
                memory_type="knowledge_base"
            )

            memory_id = store_result["memory_id"]
            print(f"✓ Stored document (memory_id: {memory_id})")

            # Write with metadata included
            write_result = self.store.write_document_to_file(
                memory_id=memory_id,
                output_path=None,
                include_metadata=True  # Enable metadata
            )

            assert write_result["success"], "Write should succeed"

            file_path = write_result["file_path"]
            print(f"✓ File written: {file_path}")

            # Read file content
            with open(file_path, 'r', encoding='utf-8') as f:
                file_content = f.read()

            print(f"\n  File content analysis:")

            # Verify YAML frontmatter exists
            assert file_content.startswith("---"), "File should start with YAML frontmatter delimiter"

            # Split into frontmatter and content
            parts = file_content.split("---", 2)
            assert len(parts) >= 3, "Should have opening ---, frontmatter, closing ---"

            frontmatter = parts[1]
            document_content = parts[2]

            print(f"    - Starts with '---': Yes")
            print(f"    - Has frontmatter: Yes ({len(frontmatter)} characters)")
            print(f"    - Has content after: Yes ({len(document_content)} characters)")

            # Verify required fields in frontmatter
            required_fields = [
                "memory_id:",
                "title:",
                "memory_type:",
                "agent_id:",
                "session_id:",
                "tags:",
                "created_at:"
            ]

            print(f"\n  Frontmatter fields:")
            for field in required_fields:
                present = field in frontmatter
                print(f"    - {field:20s} {'✓' if present else '✗'}")
                assert present, f"Frontmatter should contain {field}"

            # Verify specific values
            assert str(memory_id) in frontmatter, "Should contain correct memory_id"
            assert "Metadata Test Document" in frontmatter, "Should contain correct title"
            assert self.test_agent_id in frontmatter, "Should contain agent_id"
            assert self.test_session_id in frontmatter, "Should contain session_id"

            # Verify content follows frontmatter
            assert content.strip() in document_content.strip(), \
                "Document content should follow frontmatter"

            print(f"\n  ✓ Valid YAML format: Yes")
            print(f"  ✓ All required fields present: Yes")
            print(f"  ✓ Content follows frontmatter: Yes")

            print(f"\n✅ TEST CASE 7 PASSED: Metadata frontmatter generated correctly")
            return True

        except Exception as e:
            print(f"\n✗ TEST CASE 7 FAILED: {str(e)}")
            import traceback
            traceback.print_exc()
            return False

    # ===================================================================
    # Test Case 8: Token Estimation Accuracy
    # ===================================================================

    def test_token_estimation_accuracy(self) -> bool:
        """
        Test Case 8: Token Estimation Accuracy

        - Estimate tokens for various content sizes
        - Compare tiktoken vs fallback
        - Verify accuracy within reasonable margin
        - Performance is acceptable
        """
        print("\n" + "="*70)
        print("TEST CASE 8: Token Estimation Accuracy")
        print("="*70)

        try:
            # Test various content sizes
            test_cases = [
                ("Small", self.create_small_document()),
                ("Medium", self.create_medium_document()),
                ("Large", self.create_large_document())
            ]

            print(f"\n  Token estimation tests:")

            for name, content in test_cases:
                # Get character count
                char_count = len(content)

                # Estimate tokens using the store's method
                estimated_tokens = self.store._estimate_tokens(content)

                # Calculate expected range (1 token ≈ 4 characters)
                expected_min = char_count // 5  # Conservative estimate
                expected_max = char_count // 3  # Liberal estimate

                print(f"\n  {name} document:")
                print(f"    - Characters: {char_count:,}")
                print(f"    - Estimated tokens: {estimated_tokens:,}")
                print(f"    - Expected range: {expected_min:,} - {expected_max:,}")
                print(f"    - Chars per token: {char_count / estimated_tokens:.2f}")

                # Verify estimation is reasonable
                assert estimated_tokens > 0, "Should return positive token count"
                assert estimated_min <= estimated_tokens <= expected_max, \
                    f"Token estimate {estimated_tokens} outside reasonable range [{expected_min}, {expected_max}]"

            # Test edge cases
            print(f"\n  Edge cases:")

            # Empty string
            empty_tokens = self.store._estimate_tokens("")
            print(f"    - Empty string: {empty_tokens} tokens")
            assert empty_tokens == 0, "Empty string should be 0 tokens"

            # Single character
            single_char_tokens = self.store._estimate_tokens("a")
            print(f"    - Single char: {single_char_tokens} tokens")
            assert single_char_tokens >= 0, "Single char should be >= 0 tokens"

            # Special characters
            special_tokens = self.store._estimate_tokens("!@#$%^&*()")
            print(f"    - Special chars: {special_tokens} tokens")
            assert special_tokens >= 0, "Special chars should be >= 0 tokens"

            print(f"\n  ✓ All estimates within reasonable range")
            print(f"  ✓ Edge cases handled correctly")

            print(f"\n✅ TEST CASE 8 PASSED: Token estimation is accurate")
            return True

        except Exception as e:
            print(f"\n✗ TEST CASE 8 FAILED: {str(e)}")
            import traceback
            traceback.print_exc()
            return False

    # ===================================================================
    # Test Case 9: Error Code Coverage
    # ===================================================================

    def test_error_code_coverage(self) -> bool:
        """
        Test Case 9: Error Code Coverage

        - Test MEMORY_NOT_FOUND
        - Test INVALID_PATH (if possible)
        - Test INVALID_PARAMETER
        - Verify error response format
        - Error messages are actionable
        """
        print("\n" + "="*70)
        print("TEST CASE 9: Error Code Coverage")
        print("="*70)

        all_passed = True

        try:
            # Error 1: MEMORY_NOT_FOUND
            print(f"\n  Testing MEMORY_NOT_FOUND error:")

            result = self.store.write_document_to_file(
                memory_id=999999999,
                output_path=None
            )

            print(f"    - success: {result.get('success')}")
            print(f"    - error: {result.get('error')}")
            print(f"    - message: {result.get('message')[:80]}...")

            assert result.get("success") == False, "Should fail"
            assert result.get("error") == "MEMORY_NOT_FOUND", "Should return MEMORY_NOT_FOUND"
            assert "message" in result, "Should have error message"

            print(f"    ✓ MEMORY_NOT_FOUND working correctly")

        except Exception as e:
            print(f"    ✗ MEMORY_NOT_FOUND test failed: {e}")
            all_passed = False

        try:
            # Error 2: INVALID_PATH (relative path)
            print(f"\n  Testing INVALID_PATH error:")

            # Create a test document first
            content = self.create_small_document()
            store_result = self.store_test_memory(
                content=content,
                title="Error Test Document",
                memory_type="knowledge_base"
            )
            memory_id = store_result["memory_id"]

            result = self.store.write_document_to_file(
                memory_id=memory_id,
                output_path="relative/path/to/file.md"  # Relative path (invalid)
            )

            print(f"    - success: {result.get('success')}")
            print(f"    - error: {result.get('error')}")
            print(f"    - message: {result.get('message')[:80]}...")

            assert result.get("success") == False, "Should fail"
            assert result.get("error") == "INVALID_PATH", "Should return INVALID_PATH"
            assert "absolute" in result.get("message", "").lower(), \
                "Error message should mention absolute path"

            print(f"    ✓ INVALID_PATH working correctly")

        except Exception as e:
            print(f"    ✗ INVALID_PATH test failed: {e}")
            all_passed = False

        try:
            # Error 3: INVALID_PARAMETER (invalid format)
            print(f"\n  Testing INVALID_PARAMETER error:")

            result = self.store.write_document_to_file(
                memory_id=memory_id,
                output_path=None,
                format="invalid_format"  # Invalid format
            )

            print(f"    - success: {result.get('success')}")
            print(f"    - error: {result.get('error')}")
            print(f"    - message: {result.get('message')[:80]}...")

            assert result.get("success") == False, "Should fail"
            assert result.get("error") == "INVALID_PARAMETER", "Should return INVALID_PARAMETER"
            assert "format" in result.get("message", "").lower(), \
                "Error message should mention format parameter"

            print(f"    ✓ INVALID_PARAMETER working correctly")

        except Exception as e:
            print(f"    ✗ INVALID_PARAMETER test failed: {e}")
            all_passed = False

        # Summary
        if all_passed:
            print(f"\n  ✓ All error codes working correctly")
            print(f"  ✓ Error messages are clear and actionable")
            print(f"\n✅ TEST CASE 9 PASSED: Error handling complete")
        else:
            print(f"\n✗ TEST CASE 9 PARTIALLY FAILED: Some error codes not working")

        return all_passed

    # ===================================================================
    # Test Case 10: Integration with Search Tools
    # ===================================================================

    def test_integration_with_search_tools(self) -> bool:
        """
        Test Case 10: Integration with Search Tools

        - Test search_knowledge_base_full_documents
        - Test search_reports_full_documents
        - Test search_working_memory_full_documents
        - All return consistent size warnings
        - Response budget tracking works
        """
        print("\n" + "="*70)
        print("TEST CASE 10: Integration with Search Tools")
        print("="*70)

        all_passed = True

        # Create large documents for each memory type
        memory_types = ["knowledge_base", "reports", "working_memory"]
        large_content = self.create_large_document()

        print(f"\n  Creating large documents for each memory type...")

        memory_map = {}
        for mem_type in memory_types:
            store_result = self.store_test_memory(
                content=large_content,
                title=f"Integration Test {mem_type.replace('_', ' ').title()}",
                memory_type=mem_type,
                auto_chunk=False
            )
            memory_map[mem_type] = store_result["memory_id"]
            print(f"    - {mem_type}: memory_id={store_result['memory_id']}")

        # Test each search tool
        print(f"\n  Testing search tools:")

        for mem_type in memory_types:
            try:
                print(f"\n  {mem_type}:")

                # Perform search
                search_result = self.store.search_with_granularity(
                    query="Integration Test",
                    memory_type=mem_type,
                    granularity="coarse",
                    agent_id=self.test_agent_id,
                    session_id=self.test_session_id,
                    limit=5
                )

                assert search_result["success"], f"{mem_type} search should succeed"
                assert len(search_result["results"]) > 0, f"{mem_type} should return results"

                # Find our test document
                doc = None
                for result in search_result["results"]:
                    if result["memory_id"] == memory_map[mem_type]:
                        doc = result
                        break

                assert doc is not None, f"Should find {mem_type} document"

                # Verify size warning fields
                has_estimated_tokens = doc.get("estimated_tokens") is not None
                has_exceeds_limit = doc.get("exceeds_response_limit") is not None
                has_size_warning = doc.get("size_warning") is not None

                # Verify response budget info
                has_response_tokens = search_result.get("response_tokens") is not None
                has_budget_info = search_result.get("response_budget_info") is not None

                print(f"    Document fields:")
                print(f"      - estimated_tokens: {'✓' if has_estimated_tokens else '✗'}")
                print(f"      - exceeds_response_limit: {'✓' if has_exceeds_limit else '✗'}")
                print(f"      - size_warning: {'✓' if has_size_warning else '✗'}")

                print(f"    Response fields:")
                print(f"      - response_tokens: {'✓' if has_response_tokens else '✗'}")
                print(f"      - response_budget_info: {'✓' if has_budget_info else '✗'}")

                # Assertions
                assert has_estimated_tokens, f"{mem_type}: Missing estimated_tokens"
                assert has_exceeds_limit, f"{mem_type}: Missing exceeds_response_limit"
                assert has_size_warning, f"{mem_type}: Missing size_warning"
                assert has_response_tokens, f"{mem_type}: Missing response_tokens"
                assert has_budget_info, f"{mem_type}: Missing response_budget_info"

                # Verify size warning structure
                assert doc["exceeds_response_limit"] == True, f"{mem_type}: Should exceed limit"
                assert doc["size_warning"]["is_too_large"] == True, f"{mem_type}: Should be marked too large"
                assert "write_document_to_file" in doc["size_warning"]["recommended_action"], \
                    f"{mem_type}: Should recommend write_document_to_file"

                print(f"    ✓ {mem_type} working correctly")

            except Exception as e:
                print(f"    ✗ {mem_type} failed: {e}")
                all_passed = False

        if all_passed:
            print(f"\n  ✓ All three search tools working correctly")
            print(f"  ✓ Consistent size warnings across tools")
            print(f"  ✓ Response budget tracking present")
            print(f"\n✅ TEST CASE 10 PASSED: Search tool integration complete")
        else:
            print(f"\n✗ TEST CASE 10 FAILED: Some search tools not working correctly")

        return all_passed


# ===================================================================
# Main Test Runner
# ===================================================================

def main():
    """Run all tests in the comprehensive test suite."""

    print("="*70)
    print("COMPREHENSIVE TEST SUITE: MCP LARGE RESPONSE HANDLING")
    print("="*70)
    print("\nThis test suite validates the complete implementation of:")
    print("  - write_document_to_file MCP tool")
    print("  - Size warning features in search results")
    print("  - Token estimation and response budgeting")
    print("  - Error handling and edge cases")
    print("="*70)

    # Initialize test suite
    try:
        suite = TestLargeResponseHandling()
    except Exception as e:
        print(f"\n❌ FATAL: Could not initialize test suite: {e}")
        return 1

    # Define all test cases
    test_cases = [
        ("Test Case 1: Small Document (Direct Response)",
         suite.test_small_document_full_content),
        ("Test Case 2: Large Document (File Write)",
         suite.test_large_document_with_warning),
        ("Test Case 3: Auto-Generated Path",
         suite.test_auto_generated_path),
        ("Test Case 4: Custom Path",
         suite.test_custom_path),
        ("Test Case 5: Invalid Memory ID",
         suite.test_invalid_memory_id),
        ("Test Case 6: Chunked Document Reconstruction",
         suite.test_chunked_document_reconstruction),
        ("Test Case 7: Metadata Frontmatter",
         suite.test_metadata_frontmatter),
        ("Test Case 8: Token Estimation Accuracy",
         suite.test_token_estimation_accuracy),
        ("Test Case 9: Error Code Coverage",
         suite.test_error_code_coverage),
        ("Test Case 10: Integration with Search Tools",
         suite.test_integration_with_search_tools),
    ]

    # Run all tests
    results = []

    for test_name, test_func in test_cases:
        try:
            passed = test_func()
            results.append((test_name, passed, None))
        except Exception as e:
            print(f"\n✗ {test_name} FAILED with exception:")
            import traceback
            error_msg = traceback.format_exc()
            print(error_msg)
            results.append((test_name, False, str(e)))

    # Clean up
    suite.cleanup()

    # Print summary
    print("\n" + "="*70)
    print("TEST SUITE SUMMARY")
    print("="*70)

    passed_count = sum(1 for _, passed, _ in results if passed)
    total_count = len(results)

    for test_name, passed, error in results:
        status = "✅ PASSED" if passed else "✗ FAILED"
        print(f"{status}: {test_name}")
        if error and not passed:
            print(f"          Error: {error[:100]}...")

    print(f"\n" + "="*70)
    print(f"FINAL RESULT: {passed_count}/{total_count} tests passed")
    print("="*70)

    if passed_count == total_count:
        print("\n🎉 ALL TESTS PASSED! Implementation is ready for production.")
        return 0
    else:
        print(f"\n⚠ {total_count - passed_count} test(s) failed. Please review and fix.")
        return 1


if __name__ == "__main__":
    sys.exit(main())
