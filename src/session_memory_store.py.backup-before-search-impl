"""
Session Memory Store with Vector Search
========================================

Session-scoped memory management with semantic search via sqlite-vec.
"""

import os
import re
import json
import time
import sqlite3
import hashlib
import logging
from datetime import datetime, timezone
from pathlib import Path
from typing import Any
import tempfile

# Initialize logger
logger = logging.getLogger(__name__)

try:
    import tiktoken
    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False

try:
    import yaml
    YAML_AVAILABLE = True
except ImportError:
    YAML_AVAILABLE = False

try:
    import sqlite_vec
    SQLITE_VEC_AVAILABLE = True
except ImportError:
    SQLITE_VEC_AVAILABLE = False

from .chunking import DocumentChunker, ChunkingConfig
from .memory_types import ContentFormat, ChunkEntry, get_memory_type_config

# Import modular operations
from .storage import StorageOperations
from .search import SearchOperations
from .maintenance import MaintenanceOperations
from .chunking_storage import ChunkingStorageOperations
from .exceptions import (
    VectorMemoryException, ValidationError, MemoryError,
    SearchError, ChunkingError, DatabaseError, DatabaseLockError
)
from .retry_utils import exponential_backoff, retry_on_lock



# Valid memory types
VALID_MEMORY_TYPES = [
    "session_context", "input_prompt", "system_memory", "reports",
    "report_observations", "working_memory", "knowledge_base"
]


class SessionMemoryStore:
    """
    Session-scoped memory storage with vector search.

    Supports hierarchical chunking, embedding, and multi-granularity search.
    """

    def __init__(self, db_path: str = None):
        """
        Initialize session memory store.

        Args:
            db_path: Path to SQLite database (defaults to ./memory/agent_session_memory.db)
        """
        if db_path is None:
            # Default to ./memory/agent_session_memory.db
            memory_dir = Path.cwd() / "memory" / "memory"
            memory_dir.mkdir(parents=True, exist_ok=True)
            db_path = str(memory_dir / "agent_session_memory.db")

        self.db_path = db_path

        # Initialize database schema
        self._init_schema()

        # Initialize chunker (lazily, only when needed)
        self._chunker = None

        # Initialize embedding model (lazily, only when needed)
        self._embedding_model = None


        # Initialize operation modules
        self.storage = StorageOperations(self)
        self.search = SearchOperations(self)
        self.maintenance = MaintenanceOperations(self)
        self.chunking = ChunkingStorageOperations(self)

    # ======================
    # LAZY-LOADED PROPERTIES
    # ======================

    @property
    def token_encoder(self):
        """Cached token encoder instance."""
        if not hasattr(self, '_token_encoder'):
            if TIKTOKEN_AVAILABLE:
                try:
                    import tiktoken
                    self._token_encoder = tiktoken.get_encoding("cl100k_base")
                except Exception as e:
                    logger.warning(f"Failed to initialize tiktoken: {e}")
                    self._token_encoder = None
            else:
                self._token_encoder = None
        return self._token_encoder

    def count_tokens(self, text: str) -> int:
        """Count tokens in text."""
        if self.token_encoder:
            return len(self.token_encoder.encode(text))
        # Fallback: rough estimate (1 token â‰ˆ 4 characters)
        return len(text) // 4

    # ======================
    # DATABASE CONNECTION & SCHEMA
    # ======================

    def _get_connection(self) -> sqlite3.Connection:
        """Create a new database connection with vector search enabled."""
        conn = sqlite3.connect(self.db_path, timeout=30.0)
        conn.row_factory = sqlite3.Row
        conn.execute("PRAGMA journal_mode=WAL")
        conn.execute("PRAGMA synchronous=NORMAL")
        conn.execute("PRAGMA busy_timeout=30000")
        conn.execute("PRAGMA foreign_keys=ON")

        # Enable vector search (load sqlite-vec)
        if SQLITE_VEC_AVAILABLE:
            try:
                conn.enable_load_extension(True)
                sqlite_vec.load(conn)
                conn.enable_load_extension(False)
            except Exception as e:
                logger.warning(f"Failed to load sqlite-vec: {e}")

        return conn

    def _init_schema(self):
        """Initialize database schema with retry logic."""
        from .db_migrations import ensure_schema_up_to_date
        ensure_schema_up_to_date(self.db_path)

    # ======================
    # LAZY-LOADED COMPONENTS
    # ======================

    @property
    def chunker(self) -> 'DocumentChunker':
        """Lazy initialization of chunker"""
        if self._chunker is None:
            self._chunker = DocumentChunker()
        return self._chunker

    @property
    def token_encoder(self) -> Any:
        """Cached token encoder instance"""
        if not hasattr(self, '_token_encoder'):
            if TIKTOKEN_AVAILABLE:
                try:
                    import tiktoken
                    self._token_encoder = tiktoken.get_encoding("cl100k_base")
                except Exception as e:
                    logger.warning(f"Failed to initialize tiktoken: {e}")
                    self._token_encoder = None
            else:
                self._token_encoder = None
        return self._token_encoder

    @property
    def embedding_model(self) -> Any:
        """Lazy-load embedding model for semantic search."""
        if self._embedding_model is None:
            try:
                from sentence_transformers import SentenceTransformer
                self._embedding_model = SentenceTransformer(
                    'all-MiniLM-L6-v2',
                    device='cpu'
                )
                logger.info("Embedding model loaded successfully")
            except Exception as e:
                logger.warning(f"Failed to load embedding model: {e}")
                self._embedding_model = False
        return self._embedding_model if self._embedding_model is not False else None


    # ======================
    # PUBLIC API (Delegates to modules)
    # ======================

    def store_memory(self, *args, **kwargs) -> dict[str, Any]:
        """Store a memory entry."""
        return self.storage.store_memory(*args, **kwargs)

    def search_memories(self, *args, **kwargs) -> dict[str, Any]:
        """Search memories with filters."""
        return self.search.search_memories(*args, **kwargs)

    def search_with_granularity(self, *args, **kwargs) -> dict[str, Any]:
        """Search with specific granularity."""
        return self.search.search_with_granularity(*args, **kwargs)

    def expand_chunk_context(self, *args, **kwargs) -> dict[str, Any]:
        """Expand chunk context."""
        return self.chunking.expand_chunk_context(*args, **kwargs)

    def get_memory(self, *args, **kwargs) -> dict[str, Any]:
        """Get memory by ID."""
        return self.storage.get_memory(*args, **kwargs)

    def delete_memory(self, *args, **kwargs) -> dict[str, Any]:
        """Delete memory."""
        return self.storage.delete_memory(*args, **kwargs)

    def reconstruct_document(self, *args, **kwargs) -> dict[str, Any]:
        """Reconstruct document from chunks."""
        return self.storage.reconstruct_document(*args, **kwargs)

    def write_document_to_file(self, *args, **kwargs) -> dict[str, Any]:
        """Write document to file."""
        return self.storage.write_document_to_file(*args, **kwargs)

    def load_session_context_for_task(self, *args, **kwargs) -> dict[str, Any]:
        """Load session context."""
        return self.search.load_session_context_for_task(*args, **kwargs)

    def list_memories(self, *args, **kwargs) -> dict[str, Any]:
        """List memories."""
        return self.maintenance.list_memories(*args, **kwargs)

    def get_session_stats(self, *args, **kwargs) -> dict[str, Any]:
        """Get session statistics."""
        return self.maintenance.get_session_stats(*args, **kwargs)

    # ======================
    # CORE IMPLEMENTATION METHODS
    # ======================

    def _store_memory_impl(
        self,
        memory_type: str,
        agent_id: str,
        session_id: str,
        content: str,
        session_iter: int = 1,
        task_code: str = None,
        title: str = None,
        description: str = None,
        tags: list[str] = None,
        metadata: dict[str, Any] = None,
        auto_chunk: bool = None,
        embedding: list[float] = None
    ) -> dict[str, Any]:
        """
        Store a memory entry with optional chunking and embedding.

        Args:
            memory_type: Type of memory (session_context, input_prompt, etc.)
            agent_id: Agent identifier
            session_id: Session identifier
            content: Memory content
            session_iter: Session iteration number
            task_code: Optional task code
            title: Optional title
            description: Optional description
            tags: Optional list of tags
            metadata: Optional metadata dict
            auto_chunk: Enable automatic chunking (defaults based on memory_type)
            embedding: Optional pre-computed embedding vector

        Returns:
            Dict with success status and memory details
        """
        try:
            # Validate memory type
            if memory_type not in VALID_MEMORY_TYPES:
                return {
                    "success": False,
                    "memory_id": None,
                    "memory_type": None,
                    "agent_id": None,
                    "session_id": None,
                    "content_hash": None,
                    "chunks_created": None,
                    "created_at": None,
                    "error": "Invalid memory type",
                    "message": f"Memory type must be one of: {VALID_MEMORY_TYPES}"
                }

            # Validate agent_id (must not be empty)
            if not agent_id or agent_id.strip() == "":
                return {
                    "success": False,
                    "memory_id": None,
                    "memory_type": None,
                    "agent_id": None,
                    "session_id": None,
                    "content_hash": None,
                    "chunks_created": None,
                    "created_at": None,
                    "error": "Invalid agent_id",
                    "message": "agent_id cannot be empty"
                }

            # Validate session_id (must not be empty)
            if not session_id or session_id.strip() == "":
                return {
                    "success": False,
                    "memory_id": None,
                    "memory_type": None,
                    "agent_id": None,
                    "session_id": None,
                    "content_hash": None,
                    "chunks_created": None,
                    "created_at": None,
                    "error": "Invalid session_id",
                    "message": "session_id cannot be empty"
                }

            # Get memory type config
            config = get_memory_type_config(memory_type)

            # Determine if we should chunk (default based on memory type)
            if auto_chunk is None:
                auto_chunk = config.get('auto_chunk', False)

            # Generate content hash
            content_hash = hashlib.sha256(content.encode()).hexdigest()

            # Prepare timestamps
            now = datetime.now(timezone.utc).isoformat()

            # Prepare tags and metadata as JSON
            tags_json = json.dumps(tags if tags else [])
            metadata_json = json.dumps(metadata if metadata else {})

            # CRITICAL FIX: Do expensive operations (chunking, embedding) BEFORE opening connection
            # This prevents holding database locks during long-running operations
            chunks = []
            chunks_created = 0
            if auto_chunk:
                chunk_metadata = {
                    'memory_type': memory_type,
                    'title': title or 'Untitled',
                    'enable_enrichment': True
                }

                # Chunk document (use placeholder memory_id=0, will update after insert)
                chunks = self.chunker.chunk_document(content, 0, chunk_metadata)

                # Generate embeddings for all chunks in batch (10-50x faster than sequential)
                # FIX: Always try to generate embeddings if chunks exist
                # The embedding_model property will handle lazy loading and error cases
                if chunks:
                    chunk_texts = [chunk.content for chunk in chunks]
                    try:
                        # Use the property (not _embedding_model) to trigger lazy loading
                        model = self.embedding_model
                        if model is not None:
                            embeddings = model.encode(chunk_texts, batch_size=32, show_progress_bar=False)
                            # Convert embeddings to bytes and attach to chunks
                            for i, chunk in enumerate(chunks):
                                chunk.embedding = embeddings[i].tobytes()
                            logger.info(f"Generated {len(embeddings)} chunk embeddings")
                        else:
                            logger.warning("Embedding model not available, chunks will not have embeddings")
                    except Exception as e:
                        logger.warning(f"Failed to generate embeddings for chunks: {e}")
                        # Continue without embeddings

            # NOW open connection - all expensive operations are done
            conn = self._get_connection()

            try:
                cursor = conn.execute("""
                    INSERT INTO session_memories
                    (memory_type, agent_id, session_id, session_iter, task_code, content,
                     title, description, tags, metadata, content_hash, created_at, updated_at, accessed_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    memory_type, agent_id, session_id, session_iter, task_code, content,
                    title, description, tags_json, metadata_json, content_hash, now, now, now
                ))

                memory_id = cursor.lastrowid

                # Store embedding if provided
                if embedding:
                    conn.execute("""
                        INSERT INTO vec_session_search (memory_id, embedding)
                        VALUES (?, ?)
                    """, (memory_id, json.dumps(embedding).encode()))

                # Insert pre-computed chunks
                for chunk in chunks:
                    # Update parent_id to actual memory_id
                    chunk.parent_id = memory_id
                    conn.execute("""
                        INSERT INTO memory_chunks
                        (parent_id, chunk_index, content, chunk_type, start_char, end_char,
                         token_count, header_path, level, content_hash, created_at,
                         parent_title, section_hierarchy, granularity_level, chunk_position_ratio,
                         sibling_count, depth_level, contains_code, contains_table, keywords,
                         original_content, is_contextually_enriched, embedding)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, (
                        chunk.parent_id, chunk.chunk_index, chunk.content, chunk.chunk_type,
                        chunk.start_char, chunk.end_char, chunk.token_count, chunk.header_path,
                        chunk.level, chunk.content_hash, chunk.created_at,
                        chunk.parent_title, chunk.section_hierarchy, chunk.granularity_level,
                        chunk.chunk_position_ratio, chunk.sibling_count, chunk.depth_level,
                        chunk.contains_code, chunk.contains_table, json.dumps(chunk.keywords),
                        chunk.original_content, chunk.is_contextually_enriched,
                        chunk.embedding
                    ))

                    # Store chunk embedding in vector search if available
                    if chunk.embedding:
                        chunk_id = conn.execute("SELECT last_insert_rowid()").fetchone()[0]
                        conn.execute("""
                            INSERT INTO vec_chunk_search (chunk_id, embedding)
                            VALUES (?, ?)
                        """, (chunk_id, chunk.embedding))

                chunks_created = len(chunks)

                conn.commit()
                conn.close()

                return {
                    "success": True,
                    "memory_id": memory_id,
                    "memory_type": memory_type,
                    "agent_id": agent_id,
                    "session_id": session_id,
                    "content_hash": content_hash,
                    "chunks_created": chunks_created,
                    "created_at": now,
                    "message": f"Memory stored successfully with ID: {memory_id}",
                    "error": None
                }

            except Exception as e:
                conn.rollback()
                conn.close()
                raise

        except Exception as e:
            logger.error(f"Failed to store memory: {e}", exc_info=True)
            return {
                "success": False,
                "memory_id": None,
                "memory_type": None,
                "agent_id": None,
                "session_id": None,
                "content_hash": None,
                "chunks_created": None,
                "created_at": None,
                "error": "Storage failed",
                "message": str(e)
            }

    def _search_memories_impl(
        self,
        memory_type: str = None,
        agent_id: str = None,
        session_id: str = None,
        session_iter: int = None,
        task_code: str = None,
        query: str = None,
        limit: int = 3,
        latest_first: bool = True
    ) -> dict[str, Any]:
        """
        Search memories with optional filters.

        Args:
            memory_type: Optional memory type filter
            agent_id: Optional agent filter
            session_id: Optional session filter
            session_iter: Optional iteration filter
            task_code: Optional task code filter
            query: Optional semantic search query
            limit: Maximum results
            latest_first: Sort by newest first

        Returns:
            Dict with search results
        """
        try:
            conn = self._get_connection()

            # Build query
            conditions = []
            params = []

            if memory_type:
                conditions.append("memory_type = ?")
                params.append(memory_type)
            if agent_id:
                conditions.append("agent_id = ?")
                params.append(agent_id)
            if session_id:
                conditions.append("session_id = ?")
                params.append(session_id)
            if session_iter is not None:
                conditions.append("session_iter = ?")
                params.append(session_iter)
            if task_code:
                conditions.append("task_code = ?")
                params.append(task_code)

            where_clause = ""
            if conditions:
                where_clause = "WHERE " + " AND ".join(conditions)

            order_clause = "ORDER BY session_iter DESC, created_at DESC" if latest_first else "ORDER BY session_iter ASC, created_at ASC"

            sql = f"""
                SELECT * FROM session_memories
                {where_clause}
                {order_clause}
                LIMIT ?
            """
            params.append(limit)

            rows = conn.execute(sql, params).fetchall()
            conn.close()

            # Format results
            results = []
            for row in rows:
                results.append({
                    "id": row[0],
                    "memory_type": row[1],
                    "agent_id": row[2],
                    "session_id": row[3],
                    "session_iter": row[4],
                    "task_code": row[5],
                    "content": row[6],
                    "title": row[7],
                    "description": row[8],
                    "tags": json.loads(row[9]) if row[9] else [],
                    "metadata": json.loads(row[10]) if row[10] else {},
                    "content_hash": row[11],
                    "created_at": row[12],
                    "updated_at": row[13],
                    "accessed_at": row[14],
                    "access_count": row[15],
                    "similarity": 2.0,  # Placeholder for scoped match
                    "source_type": "scoped"
                })

            return {
                "success": True,
                "results": results,
                "total_results": len(results),
                "query": query,
                "filters": {
                    "memory_type": memory_type,
                    "agent_id": agent_id,
                    "session_id": session_id,
                    "session_iter": session_iter,
                    "task_code": task_code
                },
                "limit": limit,
                "latest_first": latest_first,
                "error": None,
                "message": None
            }

        except Exception as e:
            logger.error(f"Search failed: {e}", exc_info=True)
            return {
                "success": False,
                "results": [],
                "total_results": 0,
                "query": query,
                "filters": {},
                "limit": limit,
                "latest_first": latest_first,
                "error": "Search failed",
                "message": str(e)
            }

    def _search_with_granularity_impl(
        self,
        memory_type: str,
        granularity: str,
        query: str,
        agent_id: str = None,
        session_id: str = None,
        session_iter: int = None,
        task_code: str = None,
        limit: int = 3,
        similarity_threshold: float = 0.7,
        auto_merge_threshold: float = 0.6
    ) -> dict[str, Any]:
        """
        Search with specific granularity level.

        Granularity levels:
        - fine: Individual chunks (<400 tokens)
        - medium: Section-level with auto-merging (400-1200 tokens)
        - coarse: Full documents

        Args:
            memory_type: Type of memory to search
            granularity: Search granularity (fine/medium/coarse)
            query: Semantic search query
            agent_id: Optional agent filter
            session_id: Optional session filter
            session_iter: Optional iteration filter
            task_code: Optional task code filter
            limit: Maximum results
            similarity_threshold: Minimum similarity (0.0-1.0)
            auto_merge_threshold: For medium, merge if >=X siblings match

        Returns:
            Dict with search results at specified granularity
        """
        # For now, this is a placeholder that calls the filtered search
        # Full semantic search requires embedding infrastructure

        if granularity == "coarse":
            # Return full documents - CRITICAL FIX: Add granularity field
            result = self.search_memories(
                memory_type=memory_type,
                agent_id=agent_id,
                session_id=session_id,
                session_iter=session_iter,
                task_code=task_code,
                query=query,
                limit=limit
            )
            # Add granularity field to match GranularSearchResult schema
            result["granularity"] = "coarse"
            return result
        elif granularity == "fine":
            # Return individual chunks
            # TODO: Implement chunk-level search
            return {
                "success": True,
                "results": [],
                "total_results": 0,
                "granularity": "fine",
                "message": "Chunk-level search requires embedding infrastructure",
                "error": None
            }
        else:  # medium
            # Return section-level results
            # TODO: Implement section-level search with auto-merging
            return {
                "success": True,
                "results": [],
                "total_results": 0,
                "granularity": "medium",
                "message": "Section-level search requires embedding infrastructure",
                "error": None
            }

    def _expand_chunk_context_impl(
        self,
        memory_id: int,
        chunk_index: int,
        context_window: int = 2
    ) -> dict[str, Any]:
        """
        Expand chunk context by retrieving surrounding chunks.

        Args:
            memory_id: Parent memory ID
            chunk_index: Target chunk index
            context_window: Number of chunks before/after to retrieve

        Returns:
            Dict with target chunk and surrounding context
        """
        try:
            conn = self._get_connection()

            # Get target chunk
            target = conn.execute("""
                SELECT * FROM memory_chunks
                WHERE parent_id = ? AND chunk_index = ?
            """, (memory_id, chunk_index)).fetchone()

            if not target:
                conn.close()
                return {
                    "success": False,
                    "memory_id": None,
                    "target_chunk_index": None,
                    "context_window": None,
                    "chunks_returned": None,
                    "expanded_content": None,
                    "chunks": None,
                    "error": "Chunk not found",
                    "message": f"No chunk found at index {chunk_index} for memory {memory_id}"
                }

            # Get surrounding chunks
            start_index = max(0, chunk_index - context_window)
            end_index = chunk_index + context_window

            chunks = conn.execute("""
                SELECT * FROM memory_chunks
                WHERE parent_id = ? AND chunk_index BETWEEN ? AND ?
                ORDER BY chunk_index ASC
            """, (memory_id, start_index, end_index)).fetchall()

            conn.close()

            # Format results
            all_chunks = []
            for chunk in chunks:
                all_chunks.append({
                    "chunk_id": chunk[0],
                    "chunk_index": chunk[2],
                    "content": chunk[3],
                    "chunk_type": chunk[4],
                    "header_path": chunk[8],
                    "level": chunk[9]
                })

            # Build expanded content
            expanded_content = "\n\n".join([c["content"] for c in all_chunks])

            return {
                "success": True,
                "memory_id": memory_id,
                "target_chunk_index": chunk_index,
                "context_window": context_window,
                "chunks_returned": len(all_chunks),
                "expanded_content": expanded_content,
                "chunks": all_chunks,
                "error": None,
                "message": None
            }

        except Exception as e:
            return {
                "success": False,
                "memory_id": None,
                "target_chunk_index": None,
                "context_window": None,
                "chunks_returned": None,
                "expanded_content": None,
                "chunks": None,
                "error": "Context expansion failed",
                "message": str(e)
            }

    def _load_session_context_for_task_impl(
        self,
        agent_id: str,
        session_id: str,
        current_task_code: str
    ) -> dict[str, Any]:
        """
        Load session context only if agent previously worked on the same task_code.

        Args:
            agent_id: Agent identifier
            session_id: Session identifier
            current_task_code: Current task code to match

        Returns:
            Dict with session context if found, empty otherwise
        """
        try:
            conn = self._get_connection()

            # Find latest session_context memory matching agent, session, and task_code
            row = conn.execute("""
                SELECT * FROM session_memories
                WHERE memory_type = 'session_context'
                AND agent_id = ?
                AND session_id = ?
                AND task_code = ?
                ORDER BY session_iter DESC, created_at DESC
                LIMIT 1
            """, (agent_id, session_id, current_task_code)).fetchone()

            conn.close()

            if not row:
                return {
                    "success": True,
                    "has_context": False,
                    "context": None,
                    "message": f"No previous context found for task_code: {current_task_code}",
                    "error": None
                }

            return {
                "success": True,
                "has_context": True,
                "context": {
                    "id": row[0],
                    "memory_type": row[1],
                    "agent_id": row[2],
                    "session_id": row[3],
                    "session_iter": row[4],
                    "task_code": row[5],
                    "content": row[6],
                    "title": row[7],
                    "description": row[8],
                    "tags": json.loads(row[9]) if row[9] else [],
                    "metadata": json.loads(row[10]) if row[10] else {},
                    "created_at": row[12],
                    "updated_at": row[13]
                },
                "message": f"Found context for task_code: {current_task_code}",
                "error": None
            }

        except Exception as e:
            logger.error(f"Failed to load session context: {e}", exc_info=True)
            return {
                "success": False,
                "has_context": False,
                "context": None,
                "message": str(e),
                "error": "Failed to load session context"
            }

    def _get_memory_impl(self, memory_id: int) -> dict[str, Any]:
        """
        Retrieve specific memory by ID.

        Args:
            memory_id: Memory identifier

        Returns:
            Dict with memory details or error
        """
        try:
            conn = self._get_connection()

            row = conn.execute("""
                SELECT * FROM session_memories WHERE id = ?
            """, (memory_id,)).fetchone()

            conn.close()

            if not row:
                return {
                    "success": False,
                    "memory": None,
                    "error": "Memory not found",
                    "message": f"No memory with ID: {memory_id}"
                }

            return {
                "success": True,
                "memory": {
                    "id": row[0],
                    "memory_type": row[1],
                    "agent_id": row[2],
                    "session_id": row[3],
                    "session_iter": row[4],
                    "task_code": row[5],
                    "content": row[6],
                    "title": row[7],
                    "description": row[8],
                    "tags": json.loads(row[9]) if row[9] else [],
                    "metadata": json.loads(row[10]) if row[10] else {},
                    "content_hash": row[11],
                    "created_at": row[12],
                    "updated_at": row[13],
                    "accessed_at": row[14],
                    "access_count": row[15]
                },
                "error": None,
                "message": None
            }

        except Exception as e:
            logger.error(f"Failed to get memory: {e}", exc_info=True)
            return {
                "success": False,
                "memory": None,
                "error": "Retrieval failed",
                "message": str(e)
            }

    def _delete_memory_impl(self, memory_id: int) -> dict[str, Any]:
        """
        Delete a memory and all associated data.

        Args:
            memory_id: Memory identifier

        Returns:
            Dict with success status
        """
        try:
            conn = self._get_connection()

            # Check if memory exists
            row = conn.execute("""
                SELECT id FROM session_memories WHERE id = ?
            """, (memory_id,)).fetchone()

            if not row:
                conn.close()
                return {
                    "success": False,
                    "memory_id": None,
                    "error": "Memory not found",
                    "message": f"No memory with ID: {memory_id}"
                }

            # Delete (cascades to chunks)
            conn.execute("DELETE FROM session_memories WHERE id = ?", (memory_id,))
            conn.commit()
            conn.close()

            return {
                "success": True,
                "memory_id": memory_id,
                "error": None,
                "message": f"Memory {memory_id} deleted successfully"
            }

        except Exception as e:
            logger.error(f"Failed to delete memory: {e}", exc_info=True)
            return {
                "success": False,
                "memory_id": None,
                "error": "Deletion failed",
                "message": str(e)
            }

    def _reconstruct_document_impl(self, memory_id: int) -> dict[str, Any]:
        """
        Reconstruct a document from its chunks.

        Args:
            memory_id: Memory identifier

        Returns:
            Dict with reconstructed content
        """
        try:
            conn = self._get_connection()

            # Get parent memory
            parent = conn.execute("""
                SELECT content, title, memory_type FROM session_memories WHERE id = ?
            """, (memory_id,)).fetchone()

            if not parent:
                conn.close()
                return {
                    "success": False,
                    "memory_id": None,
                    "content": None,
                    "title": None,
                    "memory_type": None,
                    "chunk_count": 0,
                    "error": "Memory not found",
                    "message": f"No memory with ID: {memory_id}"
                }

            # Get chunks
            chunks = conn.execute("""
                SELECT chunk_index, content, chunk_type, header_path, level, original_content
                FROM memory_chunks
                WHERE parent_id = ?
                ORDER BY chunk_index ASC
            """, (memory_id,)).fetchall()

            conn.close()

            if not chunks:
                # No chunks, return original content
                return {
                    "success": True,
                    "memory_id": memory_id,
                    "content": parent[0],
                    "title": parent[1],
                    "memory_type": parent[2],
                    "chunk_count": 0,
                    "message": "No chunks found, returning original content",
                    "error": None
                }

            # Reconstruct from chunks using original_content (index 5) if available,
            # fallback to enriched content (index 1) for backward compatibility
            reconstructed_parts = []
            for chunk in chunks:
                # Use original_content if available (not None), otherwise use content
                clean_content = chunk[5] if chunk[5] is not None else chunk[1]
                reconstructed_parts.append(clean_content)

            reconstructed_content = '\n\n'.join(reconstructed_parts)

            return {
                "success": True,
                "memory_id": memory_id,
                "content": reconstructed_content,
                "title": parent[1],
                "memory_type": parent[2],
                "chunk_count": len(chunks),
                "message": f"Document reconstructed from {len(chunks)} chunks",
                "error": None
            }

        except Exception as e:
            return {
                "success": False,
                "memory_id": None,
                "content": None,
                "title": None,
                "memory_type": None,
                "chunk_count": 0,
                "error": "Reconstruction failed",
                "message": str(e)
            }

    def _write_document_to_file_impl(
        self,
        memory_id: int,
        output_path: str = None,
        include_metadata: bool = True,
        format: str = "markdown"
    ) -> dict[str, Any]:
        """
        Write a reconstructed document to disk.

        Args:
            memory_id: Memory identifier
            output_path: Path to write file (defaults to temp file)
            include_metadata: Include YAML frontmatter metadata
            format: Output format (markdown, text)

        Returns:
            Dict with file path and status
        """
        try:
            # Reconstruct document
            doc = self.reconstruct_document(memory_id)
            if not doc["success"]:
                return doc

            # Generate output path if not provided
            if output_path is None:
                import tempfile
                suffix = ".md" if format == "markdown" else ".txt"
                fd, output_path = tempfile.mkstemp(suffix=suffix)
                os.close(fd)

            # Build content
            content_parts = []

            if include_metadata and format == "markdown":
                # Add YAML frontmatter
                metadata = {
                    "memory_id": memory_id,
                    "title": doc["title"],
                    "memory_type": doc["memory_type"],
                    "chunk_count": doc["chunk_count"]
                }
                if YAML_AVAILABLE:
                    import yaml
                    content_parts.append("---")
                    content_parts.append(yaml.dump(metadata, default_flow_style=False).strip())
                    content_parts.append("---")
                    content_parts.append("")

            content_parts.append(doc["content"])

            # Write to file
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write('\n'.join(content_parts))

            return {
                "success": True,
                "file_path": output_path,
                "memory_id": memory_id,
                "bytes_written": len('\n'.join(content_parts).encode('utf-8')),
                "message": f"Document written to {output_path}",
                "error": None
            }

        except Exception as e:
            return {
                "success": False,
                "file_path": None,
                "memory_id": None,
                "bytes_written": 0,
                "error": "Write failed",
                "message": str(e)
            }
